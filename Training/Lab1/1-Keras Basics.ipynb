{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano Introduction\n",
    "Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. \n",
    "\n",
    "#### Why theano you ask?\n",
    "- tight integration with NumPy – Use numpy.ndarray in Theano-compiled functions.\n",
    "- transparent use of a GPU – Perform data-intensive calculations up to 140x faster than with CPU.(float32 only)\n",
    "- efficient symbolic differentiation – Theano does your derivatives for function with one or many inputs.\n",
    "- speed and stability optimizations – Get the right answer for log(1+x) even when x is really tiny.\n",
    "- dynamic C code generation – Evaluate expressions faster.\n",
    "- extensive unit-testing and self-verification – Detect and diagnose many types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started\n",
    "\n",
    "### 1.1 Checking your installation\n",
    "\n",
    "See if theano imports properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Theano tensors\n",
    "Theano uses <b>tensors</b> to store data. In the examples that follow we shall see some of the common tensor types. These tensors come along with various routines attached with them. Find the list of all the tensors and routines here:\n",
    "\n",
    "http://deeplearning.net/software/theano/library/tensor/basic.html|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Adding two scalars\n",
    "Reference : http://deeplearning.net/software/theano/tutorial/adding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "\n",
    "#Data structure: scalar\n",
    "x = T.dscalar('x')\n",
    "y = T.dscalar('y')\n",
    "z = x + y\n",
    "f = function([x, y], z)\n",
    "\n",
    "# Execute the function\n",
    "\n",
    "f(4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Adding two matrices\n",
    "Reference : http://deeplearning.net/software/theano/tutorial/adding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named theano.tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a70592f833d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Data structure: matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named theano.tensor"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "\n",
    "#Data structure: matrix\n",
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "z = x + y\n",
    "f = function([x, y], z)\n",
    "\n",
    "# Execute the function\n",
    "f([[1, 2], [3, 4]], [[10, 20], [30, 40]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Exercise:\n",
    "Reference : http://deeplearning.net/software/theano/tutorial/adding.html\n",
    "\n",
    "Write a program that takes a set of 2D points as input and does the following transformation on it:\n",
    "\n",
    "- x->x+y-1 and y->y-x+1\n",
    "\n",
    "You can loop over all the points or find a matrix that does this linear transformation and multiply it with the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "# END\n",
    "\n",
    "# Execute the function\n",
    "\n",
    ">>> f([[1,4],[2,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Shared Variables\n",
    "It is also possible to make a function with an internal state. For example, let’s say we want to make an accumulator: at the beginning, the state is initialized to zero. Then, on each function call, the state is incremented by the function’s argument.\n",
    "\n",
    "First let’s define the accumulator function. It adds its argument to the internal state, and returns the old state value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import shared\n",
    "state = shared(0)\n",
    "inc = T.iscalar('inc')\n",
    "accumulator = function([inc], state, updates=[(state, state+inc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code introduces a few new concepts. The shared function constructs so-called shared variables. These are hybrid symbolic and non-symbolic variables whose value may be shared between multiple functions. Shared variables can be used in symbolic expressions just like the objects returned by dmatrices(...) but they also have an internal value that defines the value taken by this symbolic variable in all the functions that use it. It is called a shared variable because its value is shared between many functions. The value can be accessed and modified by the .get_value() and .set_value() methods. We will come back to this soon.\n",
    "\n",
    "The other new thing in this code is the updates parameter of function. updates must be supplied with a list of pairs of the form (shared-variable, new expression). It can also be a dictionary whose keys are shared-variables and values are the new expressions. Either way, it means “whenever this function runs, it will replace the .value of each shared variable with the result of the corresponding expression”. Above, our accumulator replaces the state‘s value with the sum of the state and the increment amount.\n",
    "\n",
    "Let’s try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(state.get_value())\n",
    "accumulator(1)\n",
    "print(state.get_value())\n",
    "accumulator(300)\n",
    "print(state.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to reset the state. Just use the .set_value() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state.set_value(-1)\n",
    "accumulator(3)\n",
    "print(state.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Random numbers\n",
    "The way to think about putting randomness into Theano’s computations is to put random variables in your graph. Theano will allocate a NumPy RandomStream object (a random number generator) for each such variable, and draw from it as necessary. We will call this sort of sequence of random numbers a random stream. Random streams are at their core shared variables, so the observations on shared variables hold here as well. Theanos’s random objects are defined and implemented in RandomStreams and, at a lower level, in RandomStreamsBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from theano import function\n",
    "srng = RandomStreams(seed=234)\n",
    "rv_u = srng.uniform((2,2))\n",
    "rv_n = srng.normal((2,2))\n",
    "f = function([], rv_u)\n",
    "g = function([], rv_n, no_default_updates=True)    #Not updating rv_n.rng\n",
    "nearly_zeros = function([], rv_u + rv_u - 2 * rv_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_val0 = f()\n",
    "f_val1 = f()  #different numbers from f_val0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Gradients\n",
    "\n",
    "Gradients are very important for all optimization problems. Here we see how to compute simple gradient. For Jacobian and Hessian see the link: http://deeplearning.net/software/theano/tutorial/gradients.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import pp\n",
    "x = T.dscalar('x')\n",
    "y = x ** 2\n",
    "gy = T.grad(y, x)\n",
    "pp(gy)  # print out the gradient prior to optimization\n",
    "f = theano.function([x], gy)\n",
    "f(4)\n",
    "numpy.allclose(f(94.2), 188.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 A simple logistic regression in Theano\n",
    "\n",
    "Read the code carefully to gain clarity on all the concepts described above.\n",
    "\n",
    "Reference:http://deeplearning.net/software/theano/tutorial/examples.html#a-real-example-logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A basic classifier based on logistic regression\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "\n",
    "N = 400                                   # training sample size\n",
    "feats = 784                               # number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 10000\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "\n",
    "# initialize the weight vector w randomly\n",
    "#\n",
    "# this and the following bias variable b\n",
    "# are shared so they keep their values\n",
    "# between training iterations (updates)\n",
    "w = theano.shared(rng.randn(feats), name=\"w\")\n",
    "\n",
    "# initialize the bias term\n",
    "b = theano.shared(0., name=\"b\")\n",
    "\n",
    "print(\"Initial model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "\n",
    "# Construct Theano expression graph\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
    "prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
    "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
    "                                          # w.r.t weight vector w and\n",
    "                                          # bias term b\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "\n",
    "print(\"Final model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "print(\"target values for D:\")\n",
    "print(D[1])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(D[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.1 Exercise\n",
    "Just for the heck of it, try the following:\n",
    "\n",
    "Q1 : What happens if we modify the cost function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 : What happens if we modify the number of training steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other functionalities that are native to Theano, but we may not be able to cover all of them here. But you can follow all of them at:\n",
    "\n",
    "http://deeplearning.net/software/theano/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's all good, but.....\n",
    "\n",
    "Well Theano happens to be a highly customizable and powerful library for machine learning but it can certainly use a lot more abstraction. With this idea in mind an easier to use wrapper was written on top of it to facilitate easier usage without much compromise in functionality. This wrapper happens be Keras. have a look...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Introduction:\n",
    "Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on <b>top of either TensorFlow or Theano</b>. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "- allows for easy and fast prototyping (through total modularity, minimalism, and extensibility).\n",
    "- supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "- supports arbitrary connectivity schemes (including multi-input and multi-output training).\n",
    "- runs seamlessly on CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started¶\n",
    "Reference:http://keras.io/\n",
    "### 1.1 Checking your Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if keras imports successfully\n",
    "from keras.models import Model\n",
    "# Import the sequential module from keras\n",
    "from keras.models import Sequential\n",
    "# Import the layers you wish to use in your net\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import Input\n",
    "# Import the optimization algorithms that you wish to use\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "# Import other utilities that help in data formatting etc.\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate Data\n",
    "We shall write a simple logistic regression here to see how Keras works. First generate some random points for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)                       # for reproducibility\n",
    "N = 400                                   # training sample size\n",
    "feat = 784                               # number of input variables\n",
    "labels=np.random.randint(low=0, high=2, size=(N,1))\n",
    "x=np.random.randn(N,feat)\n",
    "x=x.astype('float32')\n",
    "print(np.shape(labels))\n",
    "print(np.shape(x))\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "labels = np_utils.to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Building a net\n",
    "\n",
    "Initiate a sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the regression neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a non linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compiling the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x, labels,\n",
    "                    batch_size=100, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(x, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Plotting accuracy and loss\n",
    "\n",
    "We might be interested in seeing the accuracy or convergence of our model. To do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was quick. Thats how simple Keras is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise\n",
    "Write a code that reads an image and downsamples it.\n",
    "Hint: You'll need a network with an input layer and a suitable pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the basics are out of the way, lets get serious ....."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
