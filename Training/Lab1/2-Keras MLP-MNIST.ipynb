{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - Multilayer Perceptron\n",
    "### Generating an end to end simple MLP for MNIST data classification\n",
    "Wondering what is MLP??\n",
    "(Wiki to the rescue) https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
    "<img src=\"./images/MLP-1.png\" width=\"600\" height=\"400\">\n",
    "Now that we have looked into various layers available and have made ourselves comfortable with prototxt files, we will move a step ahead and design a MLP and write a solver that trains it on MNIST data. Here our focus would be on building the layers with given number of neurons, selection of appropriate non-linear activation units and using a standard classification loss function (cross entropy). For training the network, we will use the stochastic gradient descent algorithm using backpropagation. The participants are expected to modify the parameters and observe the impact on performance and training time.\n",
    "\n",
    "Here is one possible implimentation:\n",
    "\n",
    "Reference:https://github.com/fchollet/keras/issues/112#issuecomment-101079731\n",
    "\n",
    "#### The basic steps involved would be:\n",
    "\n",
    "1. Loading and pre-processing the data\n",
    "2. Building the sequential net\n",
    "3. Compiling the net\n",
    "4. Training the net\n",
    "5. Plotting the loss and accuracy (not required for actual computation but helps in understanding the process)\n",
    "6. Testing the net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the required stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "#Load the dataset\n",
    "from keras.datasets import mnist\n",
    "# Import the sequential module from keras\n",
    "from keras.models import Sequential\n",
    "# Import the layers you wish to use in your net\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "# Import the optimization algorithms that you wish to use\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "# Import other utilities that help in data formatting etc.\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set params for net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "nb_classes = 10\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load input\n",
    "\n",
    "#### About MNIST\n",
    "The MNIST database (Mixed National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. Sample images may look like:\n",
    "<img src=\"./images/mnist_digits.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "To know more:https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "Preprocess the data if required. Here we have normalized the data to fall between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "# We need to shape the data into a shape that network accepts.\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "# Here we convert the data type of data to 'float32'\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# We normalize the data to lie in the range 0 to 1.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a sequential net\n",
    "Now we shall write the layers of the network. Initialise a sequential graph as model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a fully connected input layer of size 784x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(512, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of it add a non-linear activation function and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the hidden layer of size 512 and a non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally put an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compile the net\n",
    "In keras once a model is defined you need to compile it. You would need to specify, the <b>loss function</b>, here we have use 'categorical_crossentropy', an <b>optimizer</b>, here simple gradient descent is used, and then an metric of evaluation, here 'accuracy'.\n",
    "\n",
    "Once your model looks good, configure its learning process with .compile():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plotting accuracy and loss\n",
    "\n",
    "We might be interested in seeing the accuracy or convergence of our model. To do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Testing the net\n",
    "Print the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the fun parts...\n",
    "### 8. Exercise\n",
    "\n",
    "Q1 : Can you make the confusion matrix for the above network.\n",
    "\n",
    "(Hint: What's confusion matrix?? Find here:http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your *hardwork* here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 : Try out the following variations and see how it affects loss and accuracy:\n",
    "- Batch size\n",
    "- Size of hidden layer\n",
    "- choice of activation function ('relU','sigmoid','tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your *hardwork* here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: <b>Challenge</b>: Try classification on XOR data using similar MLP.\n",
    "\n",
    "The problem can be stated as follows. Build a neural network that will produce the following truth table, called the 'exclusive or' or 'XOR' (either A or B but not both):\n",
    "\n",
    "<img src=\"./images/xor-table.png\" width=\"200\" height=\"200\">\n",
    "\n",
    "Here (X,Y) will be the input and the label would be X xor Y.\n",
    "\n",
    "Hint: You would have to downsize the network described above as XOR is just a four point(two dimentional) two class dataset as opposed to MNIST which has tens of thousands of samples with comparitively higher dimention and ten classes. But the architecture would remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your *hardwork* here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundary and verify that it looks similar to:\n",
    "<img src=\"./images/xor.png\" width=\"200\" height=\"200\">\n",
    "\n",
    "Functions that might come in handy, meshgrid, contour, linspace. (Moreover google exists for a reason!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
